{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d552bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive (4)\\BNBUSDT.csv\n",
      "archive (4)\\BNBUSDT_norm.csv\n",
      "archive (4)\\BTCUSDT.csv\n",
      "archive (4)\\BTCUSDT_norm.csv\n",
      "archive (4)\\ETHUSDT.csv\n",
      "archive (4)\\ETHUSDT_norm.csv\n",
      "archive (4)\\XRPUSDT.csv\n",
      "archive (4)\\XRPUSDT_norm.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Replace with your local folder path\n",
    "path = \"archive (4)\"\n",
    "\n",
    "for dirname, _, filenames in os.walk(path):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a455678",
   "metadata": {},
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class SimpleTradingEnv:\n",
    "    \"\"\"Simplified trading environment\"\"\"\n",
    "\n",
    "    def __init__(self, df, initial_balance=1000, lookback_window_size=50):\n",
    "        self.df = df                          # price dataset (e.g., shape: [time, assets])\n",
    "        self.initial_balance = initial_balance\n",
    "        self.lookback_window_size = lookback_window_size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.balance = self.initial_balance   # start with all cash\n",
    "        self.net_worth = self.initial_balance\n",
    "        self.prev_net_worth = self.initial_balance\n",
    "        self.current_step = self.lookback_window_size\n",
    "        self.quants = np.zeros(self.df.shape[1])  # no assets at start\n",
    "\n",
    "        # store past steps of prices (observation window)\n",
    "        self.history = deque(maxlen=self.lookback_window_size)\n",
    "        for i in range(self.lookback_window_size):\n",
    "            self.history.append(self.df[self.current_step - i - 1])\n",
    "\n",
    "        return np.array(self.history)\n",
    "\n",
    "    def step(self, action, transaction_cost=0.001):\n",
    "        \"\"\"\n",
    "        Take one step in the environment.\n",
    "        action = portfolio weights (e.g., [0.5, 0.5] for 2 assets)\n",
    "        \"\"\"\n",
    "        prices_old = self.df[self.current_step - 1]\n",
    "        self.current_step += 1\n",
    "        prices_new = self.df[self.current_step]\n",
    "\n",
    "        # asset returns\n",
    "        returns = prices_new / prices_old - 1.0\n",
    "\n",
    "        # portfolio return (dot product of weights and returns)\n",
    "        portfolio_return = np.dot(action, returns)\n",
    "\n",
    "        # update net worth\n",
    "        self.prev_net_worth = self.net_worth\n",
    "        self.net_worth *= (1 + portfolio_return)\n",
    "\n",
    "        # transaction cost penalty\n",
    "        cost = transaction_cost * self.net_worth * np.sum(np.abs(action))\n",
    "        self.net_worth -= cost\n",
    "\n",
    "        # reward: log change in net worth\n",
    "        reward = np.log(self.net_worth / self.prev_net_worth)\n",
    "\n",
    "        # done if bankrupt\n",
    "        done = self.net_worth <= self.initial_balance / 2\n",
    "\n",
    "        # update history for next state\n",
    "        self.history.append(prices_new)\n",
    "        obs = np.array(self.history)\n",
    "\n",
    "        return obs, reward, done, self.net_worth\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Step: {self.current_step}, Net Worth: {self.net_worth:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab544d5",
   "metadata": {},
   "source": [
    "Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "class SimpleAgent:\n",
    "    \"\"\"Simplified Actor-Critic trading agent\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, lr=0.001):\n",
    "        self.state_size = state_size    # shape of input (lookback_window, features)\n",
    "        self.action_size = action_size  # number of assets\n",
    "        self.lr = lr\n",
    "\n",
    "        # Build Actor (policy network → outputs portfolio weights)\n",
    "        self.actor = models.Sequential([\n",
    "            layers.Input(shape=self.state_size),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64, activation=\"relu\"),\n",
    "            layers.Dense(self.action_size, activation=\"softmax\")  # portfolio weights\n",
    "        ])\n",
    "        self.actor.compile(optimizer=optimizers.Adam(learning_rate=self.lr),\n",
    "                           loss=\"categorical_crossentropy\")\n",
    "\n",
    "        # Build Critic (value network → predicts expected return)\n",
    "        self.critic = models.Sequential([\n",
    "            layers.Input(shape=self.state_size),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64, activation=\"relu\"),\n",
    "            layers.Dense(1, activation=\"linear\")  # value estimate\n",
    "        ])\n",
    "        self.critic.compile(optimizer=optimizers.Adam(learning_rate=self.lr),\n",
    "                            loss=\"mse\")\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Choose an action (portfolio weights) given current state\"\"\"\n",
    "        state = np.expand_dims(state, axis=0)  # add batch dimension\n",
    "        action_probs = self.actor.predict(state, verbose=0)[0]\n",
    "        return action_probs\n",
    "\n",
    "    def train(self, state, action_probs, reward, next_state, done, gamma=0.99):\n",
    "        \"\"\"One-step Actor-Critic update\"\"\"\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        next_state = np.expand_dims(next_state, axis=0)\n",
    "\n",
    "        # Critic estimates\n",
    "        value = self.critic.predict(state, verbose=0)[0]\n",
    "        next_value = self.critic.predict(next_state, verbose=0)[0]\n",
    "\n",
    "        # Compute target for critic\n",
    "        target = reward + (0 if done else gamma * next_value)\n",
    "        advantage = target - value\n",
    "\n",
    "        # Train critic\n",
    "        self.critic.train_on_batch(state, np.array([target]))\n",
    "\n",
    "        # Train actor (policy gradient with advantage)\n",
    "        self.actor.train_on_batch(state, np.expand_dims(action_probs, axis=0),\n",
    "                                  sample_weight=np.array([advantage]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0dc69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, backend as K\n",
    "\n",
    "class SimpleActor:\n",
    "    \"\"\"Actor network: chooses actions (portfolio weights)\"\"\"\n",
    "    def __init__(self, input_shape, action_space, lr=0.001):\n",
    "        self.action_space = action_space\n",
    "\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Flatten()(inputs)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        outputs = layers.Dense(action_space, activation=\"softmax\")(x)\n",
    "\n",
    "        self.model = models.Model(inputs, outputs)\n",
    "        self.model.compile(optimizer=optimizers.Adam(lr), loss=\"categorical_crossentropy\")\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.model.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
    "\n",
    "    def train(self, state, action, advantage):\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        action = np.expand_dims(action, axis=0)\n",
    "        self.model.train_on_batch(state, action, sample_weight=np.array([advantage]))\n",
    "\n",
    "\n",
    "class SimpleCritic:\n",
    "    \"\"\"Critic network: evaluates state value\"\"\"\n",
    "    def __init__(self, input_shape, lr=0.001):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Flatten()(inputs)\n",
    "        x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        x = layers.Dense(32, activation=\"relu\")(x)\n",
    "        outputs = layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "        self.model = models.Model(inputs, outputs)\n",
    "        self.model.compile(optimizer=optimizers.Adam(lr), loss=\"mse\")\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.model.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
    "\n",
    "    def train(self, state, target):\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        self.model.train_on_batch(state, np.array([target]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1efb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea93575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855d596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d3937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "020fc0d8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
